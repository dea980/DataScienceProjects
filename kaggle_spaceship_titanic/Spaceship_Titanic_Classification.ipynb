{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaceShip Titanic Classification EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the following from the project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "ss_train = pd.read_csv('/content/ss_train.csv') ## this might be on the directory\n",
    "ss_test = pd.read_csv('/content/ss_test.csv')\n",
    "# Shape and preview\n",
    "#print('Train set shape:', ss_train.shape)\n",
    "#print('Test set shape:', ss_train.shape)\n",
    "#ss_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing value check\n",
    "print('Missing values from train set:')\n",
    "print(ss_train.isna().sum())\n",
    "print('Missign values from test set:')\n",
    "print(ss_test.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for training dataset\n",
    "ss_train.head(3)\n",
    "##spaceship trainset discription\n",
    "ss_train.describe()\n",
    "## for test dataset\n",
    "ss_test.head(3)\n",
    "##spaceship testset discription\n",
    "ss_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fromt this we know that there will be 6 numerical columns from the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##For the total sum graph transported or not\n",
    "## use countplot to see in bar spot\n",
    "total=float(len(ss_train['Transported']))\n",
    "t=sns.countplot(data=ss_train, x='Transported')\n",
    "for p in t.patches:\n",
    "    percentage ='{:.1f}%'.format(100*p.get_height()/total)\n",
    "    x = p.get_x()\n",
    "    y = p.get_height()\n",
    "    t.annotate(percentage, (x,y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Graphs with the relationship between transported and other cateogories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## barplot\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "totalpl =float(len(ss_train['HomePlanet']))\n",
    "pl= sns.countplot(data=ss_train, x='HomePlanet', hue='Transported', palette = 'magma')\n",
    "for p in pl.patches:\n",
    "    percentage ='{:.1f}%'.format(100*p.get_height()/totalpl)\n",
    "    x = p.get_x()\n",
    "    y = p.get_height()\n",
    "    pl.annotate(percentage, (x,y))\n",
    "plt.subplot(2,2,2)\n",
    "totald = float(len(ss_train['Destination']))\n",
    "d= sns.countplot(data=ss_train, x='Destination', hue='Transported', palette = 'inferno')\n",
    "for p in d.patches:\n",
    "    percentage ='{:.1f}%'.format(100*p.get_height()/totald)\n",
    "    x = p.get_x()\n",
    "    y = p.get_height()\n",
    "    d.annotate(percentage, (x,y))\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "totalc =float(len(ss_train['CryoSleep']))\n",
    "c= sns.countplot(data=ss_train, x='CryoSleep', hue='Transported', palette = 'viridis')\n",
    "for p in c.patches:\n",
    "    percentage ='{:.1f}%'.format(100*p.get_height()/totalc)\n",
    "    x = p.get_x()\n",
    "    y = p.get_height()\n",
    "    c.annotate(percentage, (x,y))\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "totalv =float(len(ss_train['VIP']))\n",
    "v = sns.countplot(data=ss_train, x='VIP', hue='Transported', palette = 'hot')\n",
    "for p in v.patches:\n",
    "    percentage ='{:.1f}%'.format(100*p.get_height()/totalv)\n",
    "    x = p.get_x()\n",
    "    y = p.get_height()\n",
    "    v.annotate(percentage,(x,y))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for the numberial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "# Plot expenditure features\n",
    "fig=plt.figure(figsize=(10,20))\n",
    "for i, var_name in enumerate(numeric_feats):\n",
    "    # Left plot\n",
    "    ax=fig.add_subplot(5,2,2*i+1)\n",
    "    sns.histplot(data=ss_train, x=var_name, axes=ax, bins=30, kde=False, hue='Transported')\n",
    "    ax.set_title(var_name)\n",
    "\n",
    "    # Right plot (truncated)\n",
    "    ax=fig.add_subplot(5,2,2*i+2)\n",
    "    sns.histplot(data=ss_train, x=var_name, axes=ax, bins=30, kde=True, hue='Transported')\n",
    "    plt.ylim([0,100])\n",
    "    ax.set_title(var_name)\n",
    "fig.tight_layout()  # Improves appearance a bit\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling in the Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking with \n",
    "## historgram for\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(ss_train.loc[:,'RoomService'], bins = 30, color = 'yellow')\n",
    "\n",
    "plt.xlabel(\"Room Service Bill Amount\")\n",
    "plt.subplot(2,2,2)\n",
    "plt.hist(ss_train.loc[:,'FoodCourt'], bins = 30, color = 'red')\n",
    "plt.xlabel(\"Food Court Bill Amount\")\n",
    "plt.subplot(2,2,3)\n",
    "plt.hist(ss_train.loc[:,'ShoppingMall'], bins = 30, color = 'pink')\n",
    "plt.xlabel(\"Shopping Mall Bill Amount\")\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(ss_train.loc[:,'Spa'], bins = 30, color = 'purple')\n",
    "plt.xlabel(\"Spa Bill Amount\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ss_train.loc[:,'VRDeck'], bins = 30, color = 'green')\n",
    "plt.xlabel(xlabel = 'Transported', y = \"VR Deck Bill Amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Since age is evenly distributed, which is indicated by the 50 percentile (27) being close to the mean (27), it makes sense to replace missing values in the Age column with the mean value of 27.\n",
    "For the room service, shopping mall, spa however, the data is not evenly distributed since the summary statistics show that the median value is 0 and the histogram shows the vast majority of values are near 0. Therefore, it makes sense to make the missing values 0 for this column, which happens to be the mode too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ss_train.columns:\n",
    "    if ss_train[i].isnull().sum().any():\n",
    "        if i == 'Age':\n",
    "            ss_train[i] = ss_train[i].fillna(ss_train[i].mean())\n",
    "            ss_test[i] = ss_test[i].fillna(ss_test[i].mean())\n",
    "        else:\n",
    "            ss_train[i] = ss_train[i].fillna(ss_train[i].mode()[0])\n",
    "            ss_test[i] = ss_test[i].fillna(ss_test[i].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the barplot with the cateroical column\n",
    "plt.figure(figsize = (16,12))\n",
    "plt.subplot(2,3,1)\n",
    "## with checking  difference between with/ without filling missing values.\n",
    "## with missing value plot\n",
    "## plot with HomePlanet to see the relation with Homeplanet and Transported\n",
    "totalh =float(len(ss_train['HomePlanet']))\n",
    "h = sns.countplot(x = \"HomePlanet\" , data = ss_train) #fill in missing values of homeplanet with earth since it is the most common\n",
    "for p in h.patches:\n",
    "    percentage ='{:.1f}%'.format(100*p.get_height()/totalh)\n",
    "    x = p.get_x()\n",
    "    y = p.get_height()\n",
    "    h.annotate(percentage, (x,y))\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "## plot with Destination to see the relation with Destination and Transported\n",
    "totald =float(len(ss_train['Destination']))\n",
    "d = sns.countplot(x = \"Destination\", data = ss_train) #fill in missing values of destination with TRAPPIST-1e\n",
    "for p in d.patches:\n",
    "    percentage ='{:.1f}%'.format(100*p.get_height()/totald)\n",
    "    x = p.get_x()\n",
    "    y = p.get_height()\n",
    "    d.annotate(percentage,(x,y))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_train.loc[:,'HomePlanet'] = ss_train.loc[:, 'HomePlanet'].fillna(\"Earth\")\n",
    "ss_train.loc[:, 'Destination'] = ss_train.loc[:, 'Destination'].fillna(\"TRAPPIST-1e\")\n",
    "\n",
    "##for checking the function worked in proper.\n",
    "##ss_train.isna().sum()\n",
    "## barplot after we coved with the missing values\n",
    "plt.figure(figsize = (16,12))\n",
    "plt.subplot(2,3,1)\n",
    "totalh =float(len(ss_train['HomePlanet']))\n",
    "h = sns.countplot(x = \"HomePlanet\" , data = ss_train) #fill in missing values of homeplanet with earth since it is the most common\n",
    "for p in h.patches:\n",
    "    percentage ='{:.1f}%'.format(100*p.get_height()/totalh)\n",
    "    x = p.get_x()\n",
    "    y = p.get_height()\n",
    "    h.annotate(percentage, (x,y))\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "totald =float(len(ss_train['Destination']))\n",
    "d = sns.countplot(x = \"Destination\", data = ss_train) #fill in missing values of destination with TRAPPIST-1e\n",
    "for p in d.patches:\n",
    "    percentage ='{:.1f}%'.format(100*p.get_height()/totald)\n",
    "    x = p.get_x()\n",
    "    y = p.get_height()\n",
    "    d.annotate(percentage,(x,y))\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## for ages\n",
    "# figure size\n",
    "plt.figure(figsize = (16,12))\n",
    "## To find the distribution of the Age\n",
    "plt.subplot(3,1,1)\n",
    "sns.histplot(data=ss_train, x= 'Age')\n",
    "plt.subplot(3,1,2)\n",
    "## to fine the range of the Age\n",
    "sns.boxplot(data=ss_train, x='Age', palette = [\"#fc9272\", \"#fee0d2\"])\n",
    "plt.subplot(3,1,3)\n",
    "# For the AgeS reformatt in the 0~10, 11~20 ....\n",
    "ss_traina = ss_train.copy()\n",
    "ss_traina['Age_by_decade'] = pd.cut(x=ss_traina['Age'], bins=[9,19,29,39,49,59,69,79,89,99], labels=['0s','10s','20s','30s','40s','50s','60s','70s', '80s'], right = True)\n",
    "# sns.countplot(data = ss_train, x = ss_train['Age_by_decade'], hue='Transported')\n",
    "## covering all the range of\n",
    "totala =float(len(ss_traina['Age_by_decade']))\n",
    "a = sns.countplot(data=ss_traina, x='Age_by_decade', hue='Transported', palette = 'hot')\n",
    "for p in a.patches:\n",
    "    percentage ='{:.1f}%'.format(100*p.get_height()/totala)\n",
    "    x = p.get_x()\n",
    "    y = p.get_height()\n",
    "    a.annotate(percentage,(x,y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Findning correlations between columns\n",
    "\n",
    "cor = ss_train.corr()\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.heatmap(cor, annot = True)\n",
    "rel = cor['Transported'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above correlation matrix shows there is not much correlation between the most of the variables. However, there is a slight relationship between room service and transported, spa and transported, and VRDeck and transported as the relationship is a negative correlation between -.2 and -.25 for these three relationships. We can explore these relationships further with scatter plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PCA, formating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature\n",
    "## False = 0 True = 1\n",
    "ss_trainc = ss_train.copy()\n",
    "ss_testc = ss_test.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check code\n",
    "ss_trainc.shape\n",
    "ss_train.shape\n",
    "ss_train.dtypes\n",
    "## check missing values again\n",
    "print(ss_trainc.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for handling the missing values from the cateorical labels\n",
    "# Drop qualitative/redundant/high cardinality features\n",
    "ss_trainc.set_index('PassengerId')\n",
    "ss_trainc.drop([ 'Cabin', 'Name', 'Age'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testc\n",
    "ss_testc.drop(['Cabin', 'Name','Age'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview resulting training set\n",
    "ss_trainc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_testc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passengerId = ss_test['PassengerId']\n",
    "ss_test.set_index('PassengerId')\n",
    "passengerId "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "ss_test.drop(['Cabin', 'Name','Age'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=ss_trainc['Transported'].copy().astype(int)\n",
    "X=ss_trainc.drop('Transported', axis=1).copy()\n",
    "X_test=ss_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "# Indentify numerical and categorical columns\n",
    "numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n",
    "categorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]\n",
    "\n",
    "# Scale numerical data to have mean=0 and variance=1\n",
    "numerical_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "# One-hot encode categorical data\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(drop='if_binary', handle_unknown='ignore',sparse=False))])\n",
    "\n",
    "# Combine preprocessing\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)],\n",
    "        remainder='passthrough')\n",
    "\n",
    "# Apply preprocessing\n",
    "X = ct.fit_transform(X)\n",
    "X_test = ct.transform(X_test)\n",
    "\n",
    "# Print new shape\n",
    "print('Training set shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "pca = PCA(n_components=3)\n",
    "components = pca.fit_transform(X)\n",
    "print(components.shape)\n",
    "\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    components, x=0, y=1, z=2, color=y, size=0.1*np.ones(len(X)), opacity = 1,\n",
    "    title=f'Total Explained Variance: {total_var:.2f}%',\n",
    "    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'},\n",
    "    width=800, height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main component explains only 48.65% from original data\n",
    "For finding number of main Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance (how important each additional principal component is)\n",
    "#pca = PCA().fit(X)\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "xi = np.arange(1, 1+X.shape[1], step=1)\n",
    "yi = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(xi, yi, marker='o', linestyle='--', color='b')\n",
    "\n",
    "# Aesthetics\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(1, 1+X.shape[1], step=2))\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('Explained variance by each component')\n",
    "plt.axhline(y=1, color='r', linestyle='-')\n",
    "plt.text(0.5, 0.85, '100% cut-off threshold', color = 'red')\n",
    "ax.grid(axis='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### classifier KNN, NaiveBase, Logistic Regression\n",
    "I just do it once, if you want we can do it in seperate for the method\n",
    "To briefly mention the algorithms we will use,\n",
    "Logistic Regression: Unlike linear regression which uses Least Squares, this model uses Maximum Likelihood Estimation to fit a sigmoid-curve on the target variable distribution. The sigmoid/logistic curve is commonly used when the data is questions had binary output.\n",
    "\n",
    "K-Nearest Neighbors (KNN): KNN works by selecting the majority class of the k-nearest neighbours, where the metric used is usually Euclidean distance. It is a simple and effective algorithm but can be sensitive by many factors, e.g. the value of k, the preprocessing done to the data and the metric used.\n",
    "\n",
    "Naive Bayes (NB): Naive Bayes learns how to classify samples by using Bayes' Theorem. It uses prior information to 'update' the probability of an event by incoorporateing this information according to Bayes' law. The algorithm is quite fast but a downside is that it assumes the input features are independent, which is not always the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature (change the string to the numeric)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "la = LabelEncoder()\n",
    "for i in ss_test.columns:\n",
    "    if ss_train[i].dtype == 'object' or ss_train[i].dtype == 'bool':\n",
    "        ss_train[i] = la.fit_transform(ss_train[i])\n",
    "        ss_test[i] = la.fit_transform(ss_test[i])\n",
    "\n",
    "ss_train['Transported'] = la.fit_transform(ss_train['Transported'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check train and test\n",
    "ss_train.head()\n",
    "ss_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Findning correlations between columns\n",
    "ss_trainco = ss_train.copy()\n",
    "ss_trainco.drop('Transported', axis = 1, inplace= True)\n",
    "cor = ss_trainco.corr()\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.heatmap(cor, annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_trainm = ss_train.copy()\n",
    "ss_testm = ss_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#from catboost import CatBoostClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['HomePlanet','CryoSleep','Destination','VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa','VRDeck']\n",
    "y = ss_trainm['Transported']\n",
    "X = ss_trainm.loc[:, features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0,shuffle=True, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Knn = KNeighborsClassifier()\n",
    "Logistic_regression = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "Naive_bayes = GaussianNB()\n",
    "Decision_Tree = tree.DecisionTreeClassifier()\n",
    "Random_forest = RandomForestClassifier(bootstrap= False, n_estimators=529, max_depth=15, min_samples_split=5,min_samples_leaf=4,max_features='auto',verbose=0)\n",
    "XGB = XGBClassifier(gamma= 0.8151728866167003,learning_rate= 0.031628174313413464,max_depth= 7,n_estimators= 207,subsample= 0.7781385659303335)\n",
    "LGBM = LGBMClassifier(learning_rate=0.04183147620569966,max_depth= 25, min_child_samples= 117, n_estimators= 240)\n",
    "GradientBoost = GradientBoostingClassifier(n_estimators=100)\n",
    "#CatBoost = CatBoostClassifier(objective='CrossEntropy', colsample_bylevel= 0.07587945476302646, depth= 8, boosting_type= 'Ordered', bootstrap_type= 'Bernoulli', subsample= 0.737265320016441,iterations=1000,verbose=0)\n",
    "NNMLP =MLPClassifier(hidden_layer_sizes = (20,20,), activation='relu', solver='adam',max_iter=200,verbose=0)\n",
    "models = [Knn,Logistic_regression, Naive_bayes , Decision_Tree, Random_forest, XGB, LGBM, GradientBoost,NNMLP]\n",
    "accuracy =[]\n",
    "train_time =[]\n",
    "predict_time =[]\n",
    "total_time =[]\n",
    "for model in models:\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    end_train =time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    end_predict = time.time()\n",
    "    acc_model = round(accuracy_score(y_pred, y_test) * 100, 2)\n",
    "    accuracy.append(acc_model)\n",
    "    t_time = round(end_train-start,2)\n",
    "    train_time.append(t_time)\n",
    "    p_time = round(end_predict-end_train,2)\n",
    "    predict_time.append(p_time)\n",
    "    tt_time = round(end_predict-start,2)\n",
    "    total_time.append(tt_time)\n",
    "\n",
    "\n",
    "model_name = ['KNN', 'Logistic Regression', 'Naive Bayes', 'Decision Tree', 'Random_forest', ' XGB', 'LGBM', 'GradientBoost', 'NNMLP']\n",
    "models_table = pd.DataFrame({'Model name': model_name, 'Accuracy percentage': accuracy, 'Train time' : train_time, 'Predict time': predict_time, 'Total time': total_time})\n",
    "models_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score ## for K-fold Validation\n",
    "cv_mean = []\n",
    "for model in models:\n",
    "    scores = cross_val_score(model, X, y,\n",
    "                              cv=5,\n",
    "                              scoring='accuracy')\n",
    "    cv_mean.append(scores.mean())\n",
    "\n",
    "models_validation = pd.DataFrame({'Model name': model_name, 'K-Fold validation mean scores': cv_mean})\n",
    "models_validation = models_validation.sort_values(by='K-Fold validation mean scores', ascending=False)\n",
    "models_validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick the most accurate one\n",
    "LR = LGBMClassifier(learning_rate=0.04183147620569966,max_depth= 25, min_child_samples= 117, n_estimators= 240)\n",
    "LR.fit(X_train, y_train)\n",
    "LR_prediction = LR.predict(X)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix  # For find confusion matrix\n",
    "\n",
    "# Find confusion matrix for this model:\n",
    "confusion_mat_LR = confusion_matrix(y, LR_prediction)\n",
    "confusion_mat_dataframe_LR = pd.DataFrame(confusion_mat_LR, index=[\"Transported\", \"NotTransported\"], columns=[\"Transported\", \"NotTransported\"])\n",
    "sns.heatmap(confusion_mat_dataframe_LR, annot=True, annot_kws={\"size\": 18}, fmt=\"d\")\n",
    "plt.title(\"LGBM\")\n",
    "plt.ylabel('Actual Classes')\n",
    "plt.xlabel('Predicted Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report  # For print evaluation report\n",
    "report_LR = pd.DataFrame(classification_report(y, LR_prediction, output_dict=True, target_names=[\"Transported\", \"NotTransported\"]))\n",
    "report_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prediction = LR_prediction\n",
    "best_prediction\n",
    "print(np.round(100*np.round(best_prediction).sum()/len(best_prediction),2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_test\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passengerId = ss_test['PassengerId']\n",
    "ss_test = ss_test.drop('PassengerId', axis = 1)\n",
    "print(ss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LGBMClassifier(learning_rate=0.04183147620569966,max_depth= 25, min_child_samples= 117, n_estimators= 240)\n",
    "Most = LR.fit(X_train, y_train)\n",
    "Most.predict(ss_test)\n",
    "pred1 = pd.DataFrame(Most.predict(ss_test))\n",
    "pred1.columns =['Transported']\n",
    "pred1['PassengerId'] = passengerId\n",
    "pred1 = pred1[['PassengerId', 'Transported']]\n",
    "for i in range(len(pred1)):\n",
    "  if pred1.iloc[i,1] == 1:\n",
    "    pred1.iloc[i,1] = 'True'\n",
    "  else:\n",
    "    pred1.iloc[i,1] = 'False'\n",
    "  #pred1.iloc[i,1] = pred1.iloc[i,1]\n",
    "pred1.to_csv(\"ss_submission.csv\", index =False)\n",
    "pred1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
